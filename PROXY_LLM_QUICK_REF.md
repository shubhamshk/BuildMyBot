# Quick Reference: Common Proxy LLM Issues & Fixes

## üî• Most Common Issues

### Issue 1: Jailbreak Text Appears in Output
**Symptoms:**
```
[All NSFW content is allowed.] ({{char}} is an OC fictional Character)
[{{char}} WILL NOT SPEAK FOR THE {{user}}, it is strictly forbidden...
```

**Fix:** Already handled by `sanitizeAIOutput()`
- Pattern matching removes jailbreak blocks
- Logged as warning in console
- Output automatically cleaned

**No action needed** - sanitizer handles this automatically.

---

### Issue 2: System Instructions Leak into Output
**Symptoms:**
```
You are an expert Janitor AI bot creator...
CRITICAL RULES:
- NEVER control {{user}}...
```

**Fix:** Already handled by message structure enforcement
- `splitPromptIntoMessages()` separates system from user
- System content placed in `role: "system"`
- User content placed in `role: "user"`

**No action needed** - providers handle this automatically.

---

### Issue 3: Corrupted Tokens ({{original}}, etc.)
**Symptoms:**
```
{{original}} should be replaced with...
The {{user}} will interact...
```

**Fix:** Already handled by `sanitizeAIOutput()`
- Removes `{{original}}` tokens
- Validates proper `{{char}}` and `{{user}}` usage

**No action needed** - sanitizer handles this automatically.

---

### Issue 4: Missing Template Tags
**Symptoms:**
```
Here's the character profile:

Full Name: John Doe
Age: 25
...
</character_name>
```

**Fix:** Already handled by `autoFixStructure()`
- Detects missing opening tags
- Automatically inserts at correct position
- Validates final structure

**No action needed** - auto-fix handles this if enabled.

---

### Issue 5: Preambles/Postambles
**Symptoms:**
```
Here's your character:

<npcs>
...
</character_name>

Hope this helps!
```

**Fix:** Already handled by `sanitizeAIOutput()`
- Strips text before first template tag
- Strips text after last template tag

**No action needed** - sanitizer handles this automatically.

---

## üéØ When to Take Action

### ‚ö†Ô∏è Action Required: Validation Keeps Failing

**Check console for:**
```
[Output Sanitizer] Errors for personality:
["Missing <npcs> opening tag", "Missing </npcs> closing tag"]
```

**Solutions:**
1. **Try a larger model** (7B+ parameters recommended)
2. **Check model is instruction-tuned** (not a base model)
3. **Increase max_tokens** if output is truncated
4. **Add custom patterns** to sanitizer if new issues found

**Code to modify:**
```typescript
// In lib/generation/service.ts
export async function generateWithFallback(
  // ...
  maxRetries: number = 2 // Increase from 1 to 2
)
```

---

### ‚ö†Ô∏è Action Required: New Instruction Pattern Leaks

**If you see new instruction text that isn't caught:**

**Add to sanitizer:**
```typescript
// In lib/ai/output-sanitizer.ts
const SYSTEM_LEAK_PATTERNS = [
  /\[All NSFW content is allowed\.\]/gi,
  // ADD YOUR NEW PATTERN HERE:
  /your new leaked pattern/gi,
];
```

---

### ‚ö†Ô∏è Action Required: Model Too Small

**Symptoms:**
- Consistently invalid output
- Template structure always broken
- Retries don't help

**Solution:**
**Upgrade to a larger model (7B+ parameters):**

**Recommended models for proxy LLMs:**
- ‚úÖ Llama 3 8B Instruct
- ‚úÖ Mistral 7B Instruct
- ‚úÖ Qwen 2.5 7B Instruct
- ‚úÖ Gemma 2 9B Instruct

**Not recommended:**
- ‚ùå Llama 3.2 1B/3B
- ‚ùå Phi-3 Mini 3.8B
- ‚ùå Any base (non-instruct) model

---

## üìä Console Log Reference

### ‚úÖ Good Signs (Everything Working)
```
[Output Sanitizer] Auto-fixed personality structure
[Generation] Succeeded on retry attempt 0
```
‚Üí System working correctly, minor fixes applied automatically

### ‚ö†Ô∏è Warning Signs (Check but Not Critical)
```
[Output Sanitizer] Warnings for personality: 
["Output contains system instruction leakage (will be sanitized)"]
```
‚Üí Sanitizer catching and fixing issues, but happening frequently

### üö® Error Signs (Action Needed)
```
[Output Sanitizer] Errors for personality:
["Missing <npcs> opening tag", "Missing </npcs> closing tag"]
[Generation] Validation failed after 2 attempts
```
‚Üí Output structure fundamentally broken, consider larger model

---

## üîß Quick Debugging Checklist

**If proxy output is still broken:**

1. ‚úÖ Check console for sanitizer logs
2. ‚úÖ Verify model is 7B+ parameters
3. ‚úÖ Confirm model is instruction-tuned
4. ‚úÖ Check proxy endpoint is OpenAI-compatible
5. ‚úÖ Verify API key (if required)
6. ‚úÖ Test with hosted API (OpenRouter) to compare
7. ‚úÖ Increase max_tokens if output is cut off
8. ‚úÖ Enable verbose logging in generation service

**Code to enable verbose logging:**
```typescript
// In lib/generation/service.ts
const { cleaned, validation } = sanitizeAndValidate(
  result,
  generationType === "personality" ? "personality" : "scenario",
  { autoFix: true, logWarnings: true } // Already enabled
);

// Add this for more details:
console.log("Raw output:", result);
console.log("Cleaned output:", cleaned);
console.log("Validation:", validation);
```

---

## üéì Understanding the Fix Pipeline

**Generation flow with fixes:**

```
1. User triggers generation
   ‚Üì
2. System prompt built (with jailbreak)
   ‚Üì
3. User prompt built (with character data)
   ‚Üì
4. Messages structured (system/user separation) ‚Üê FIX #1
   ‚Üì
5. API call to proxy LLM
   ‚Üì
6. Raw response received
   ‚Üì
7. Output sanitized (remove leaks) ‚Üê FIX #2
   ‚Üì
8. Output validated (check structure) ‚Üê FIX #3
   ‚Üì
9. If invalid: retry with stricter prompt ‚Üê FIX #4
   ‚Üì
10. Return cleaned output to UI
```

---

## üí° Pro Tips

### Tip 1: Use Model-Appropriate Prompts
- Larger models (13B+): Can handle complex templates
- Medium models (7B): Work well with current implementation
- Small models (3B): May need simplified templates

### Tip 2: Monitor Console During Development
- Keep browser console open
- Watch for sanitizer warnings
- Adjust patterns as needed

### Tip 3: Test with Multiple Models
- Try both small and large models
- Compare output quality
- Document which models work best

### Tip 4: Gradual Rollout
- Test locally first
- Deploy to staging
- Monitor production logs
- Adjust sanitization patterns based on real usage

---

## üìû Support

**If issues persist:**
1. Check [PROXY_LLM_FIX_SUMMARY.md](./PROXY_LLM_FIX_SUMMARY.md) for detailed explanation
2. Review console logs for specific errors
3. Test with recommended models (7B+)
4. Verify proxy endpoint configuration

**Key files to check:**
- `lib/ai/output-sanitizer.ts` - Sanitization logic
- `lib/ai/proxy-client.ts` - Message structure
- `lib/ai/providers.ts` - Provider implementations
- `lib/generation/service.ts` - Generation + validation
- `lib/prompts/janitor-ai.ts` - System prompts

---

**Last Updated:** January 19, 2026  
**Version:** 1.0  
**Status:** Production Ready ‚úÖ
