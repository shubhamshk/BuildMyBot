# Proxy LLM Output Fix - Implementation Summary

## ğŸ¯ Problem Statement

Proxy/local LLMs were producing broken, unreadable output with:
- System rules and jailbreak text leaking into final output
- Merged or malformed template sections
- Repeated guideline text like "ALL CONTENT IS ALLOWED"
- Corrupted tokens ({{original}}, {{user}} fragments)
- No separation between system instructions and generated content

## âœ… Solution Implemented

### 1. **Output Sanitization Layer** (NEW FILE)
**File:** `lib/ai/output-sanitizer.ts`

**What it does:**
- Removes leaked system instruction patterns
- Strips jailbreak text from output
- Filters instruction keywords
- Removes text before/after template tags
- Normalizes spacing and removes markdown artifacts
- Removes corrupted tokens

**Key Functions:**
- `sanitizeAIOutput()` - Main sanitization function
- `validatePersonalityOutput()` - Validates personality template structure
- `validateScenarioOutput()` - Validates scenario template structure
- `autoFixStructure()` - Attempts to auto-repair broken templates
- `sanitizeAndValidate()` - Combined sanitization + validation pipeline

**Example Usage:**
```typescript
import { sanitizeAndValidate } from "@/lib/ai/output-sanitizer";

const { cleaned, validation } = sanitizeAndValidate(
  rawOutput,
  "personality", // or "scenario" or "bio"
  { autoFix: true, logWarnings: true }
);

if (validation.valid) {
  // Use cleaned output
} else {
  console.error("Validation errors:", validation.errors);
}
```

### 2. **Message Structure Enforcement**
**File:** `lib/ai/proxy-client.ts`

**What changed:**
- Added `enforceMessageStructure()` function
- Automatically separates system, user, and assistant messages
- Prevents system prompt concatenation
- Ensures proper role boundaries

**Why this matters:**
Proxy LLMs often fail to distinguish between:
- System instructions (should never appear in output)
- User prompts (the actual generation request)
- Assistant responses (the generated content)

By enforcing role separation, we prevent jailbreak text and rules from leaking into character content.

### 3. **Provider-Level Message Splitting**
**File:** `lib/ai/providers.ts`

**What changed:**
- Added `splitPromptIntoMessages()` helper function
- All providers now properly split system/user messages
- Added output sanitization to all providers (OpenAI, Proxy, LMStudio)
- Special focus on ProxyProvider (most critical for local LLMs)

**Key improvements:**
```typescript
// BEFORE: System and user prompts concatenated
const prompt = systemPrompt + "\n\n" + userPrompt;

// AFTER: Properly structured messages
const messages = [
  { role: "system", content: systemPrompt },
  { role: "user", content: userPrompt }
];
```

### 4. **Hardened System Prompts**
**File:** `lib/prompts/janitor-ai.ts`

**What changed:**
- Added explicit "DO NOT ECHO THESE INSTRUCTIONS" warnings
- Added visual separators to make critical rules stand out
- Emphasized that output must contain ONLY the template
- Clarified that jailbreak text is part of system context, not output

**Example addition:**
```
âš ï¸ CRITICAL OUTPUT RULES (READ CAREFULLY):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. YOUR RESPONSE MUST CONTAIN ONLY THE FILLED TEMPLATE
2. DO NOT ECHO THESE INSTRUCTIONS IN YOUR OUTPUT
3. DO NOT INCLUDE EXPLANATIONS, PREAMBLES, OR POSTAMBLES
4. DO NOT INCLUDE ANY TEXT BEFORE <npcs> OR AFTER </character_name>
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### 5. **Validation & Retry Logic**
**File:** `lib/generation/service.ts`

**What changed:**
- Integrated sanitization into `generateWithFallback()`
- Added automatic validation after generation
- Retry with stricter instructions if validation fails
- Returns cleaned output even if validation fails (graceful degradation)

**Flow:**
1. Generate content
2. Sanitize output
3. Validate structure
4. If invalid and retries remain: retry with stricter prompt
5. If out of retries: return cleaned output anyway
6. Log warnings/errors for debugging

## ğŸ“‹ Files Modified

1. âœ¨ **NEW:** `lib/ai/output-sanitizer.ts` - Complete sanitization system
2. ğŸ”§ **UPDATED:** `lib/ai/proxy-client.ts` - Message structure enforcement
3. ğŸ”§ **UPDATED:** `lib/ai/providers.ts` - Provider-level sanitization + splitting
4. ğŸ”§ **UPDATED:** `lib/generation/service.ts` - Validation + retry logic
5. ğŸ”§ **UPDATED:** `lib/prompts/janitor-ai.ts` - Hardened system prompts

## ğŸš€ Testing Your Changes

### Test 1: Local LLM via Proxy
1. Open API Keys page (`/api-keys`)
2. Configure Custom Proxy with your LM Studio/Ollama endpoint
3. Create a test character
4. Generate personality profile
5. **Expected:** Clean output with no system instructions visible

### Test 2: Check Console Logs
Open browser console and look for:
- `[Output Sanitizer]` warnings about leaked instructions
- `[Generation]` logs about validation status
- Should see sanitization happening in real-time

### Test 3: Verify Template Structure
After generation, check that output:
- Starts with `<npcs>` (no preamble)
- Ends with `</character_name>` (no postamble)
- Contains no jailbreak text
- Contains no instruction keywords

## ğŸ” How to Debug Issues

### If output still contains system instructions:
1. Check console for `[Output Sanitizer]` warnings
2. Look at the patterns in `SYSTEM_LEAK_PATTERNS` array
3. Add new patterns if needed

### If validation fails repeatedly:
1. Check console for validation errors
2. May indicate model is too small (see note below)
3. Try with a larger model (7B+)

### If output is truncated:
1. Check max_tokens setting
2. Increase token limits in `getMaxTokensForType()`

## âš ï¸ Model Capability Note

**Small models (â‰¤ 3B parameters) often struggle with:**
- Following complex structured output instructions
- Separating system context from generated content
- Respecting role boundaries
- Maintaining template consistency

**For reliable proxy/local LLM generation:**
- âœ… **Recommended:** 7B+ parameter models (Llama 3 8B, Mistral 7B, Qwen 7B)
- âš ï¸ **Minimum:** 3B models with instruction tuning (may require retries)
- âŒ **Not recommended:** Base models without instruction tuning

**This sanitizer provides a safety net, but cannot fix fundamentally incoherent output.**

## ğŸ“Š Expected Outcomes

After implementing these changes:

âœ… Proxy LLM output is clean and readable  
âœ… No system or jailbreak text leaks into UI  
âœ… Personality profiles follow intended format  
âœ… Output quality matches hosted API behavior  
âœ… Same prompts work reliably across models  

## ğŸ› ï¸ Maintenance

### To add new sanitization patterns:
Edit `lib/ai/output-sanitizer.ts`:
```typescript
const SYSTEM_LEAK_PATTERNS = [
  /\[All NSFW content is allowed\.\]/gi,
  // Add new patterns here
  /your new pattern/gi,
];
```

### To adjust validation rules:
Edit validation functions in `lib/ai/output-sanitizer.ts`:
```typescript
export function validatePersonalityOutput(output: string): ValidationResult {
  // Add new validation checks here
}
```

### To modify retry behavior:
Edit `lib/generation/service.ts`:
```typescript
export async function generateWithFallback(
  // ...
  maxRetries: number = 1 // Increase for more retries
)
```

## ğŸ“ Key Design Decisions

1. **Why sanitize AFTER generation instead of BEFORE?**
   - Sanitization in prompts can't handle all edge cases
   - Post-processing is more reliable and maintainable
   - Can adapt to different model behaviors

2. **Why enforce message roles?**
   - Prevents system prompt concatenation (root cause)
   - Standard practice for OpenAI-compatible APIs
   - Models understand role boundaries better

3. **Why add retry logic?**
   - Small models may fail first attempt
   - Stricter instructions on retry often help
   - Graceful degradation if all attempts fail

4. **Why keep existing prompts intact?**
   - Backward compatibility
   - Hosted APIs still work as before
   - Only adds safety layers, doesn't change behavior

## ğŸ“ Learn More

- OpenAI Chat Completions API: https://platform.openai.com/docs/api-reference/chat
- Janitor AI Format Guide: (Internal documentation)
- LM Studio Setup: https://lmstudio.ai/docs

## ğŸ’¡ Future Improvements

Potential enhancements (not implemented yet):
- Streaming support with real-time sanitization
- Model-specific sanitization profiles
- Automatic model detection and recommendation
- User-configurable sanitization rules
- Performance metrics and quality scoring

---

**Questions or Issues?**
Check console logs for detailed sanitization and validation output.
All changes are backward-compatible and safe to deploy.
